{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2sA7Ndq9JHUIx0boeLhdc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Purity-asiko/Pree-Projects/blob/main/Sentiment_Analysis_with_BERT_(IMDB_Dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Name: Sentiment Analysis with BERT (IMDB Dataset)\n",
        "\n",
        "Goal: Classify movie reviews as positive or negative using BERT."
      ],
      "metadata": {
        "id": "3kUt1e_FWTdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFlv0nWIVUzu",
        "outputId": "b74e752b-b609-47d7-fcba-39bbe4471875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SkI6EgpjVzE0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "    word_index = imdb.get_word_index()\n",
        "    reverse_word_index = {v + 3: k for k, v in word_index.items()}\n",
        "    def decode_review(encoded):\n",
        "        return ' '.join([reverse_word_index.get(i, '?') for i in encoded])\n",
        "    x_train_text = [decode_review(x) for x in x_train[:5000]]  # Limit for speed\n",
        "    x_test_text = [decode_review(x) for x in x_test[:1000]]\n",
        "    y_train = y_train[:5000]\n",
        "    y_test = y_test[:1000]\n",
        "except Exception as e:\n",
        "    print(\"Error loading dataset:\", e)"
      ],
      "metadata": {
        "id": "MaFf2sIpV47e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    def tokenize_reviews(reviews, max_length=128):\n",
        "        return tokenizer(reviews, padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
        "    train_encodings = tokenize_reviews(x_train_text)\n",
        "    test_encodings = tokenize_reviews(x_test_text)\n",
        "except Exception as e:\n",
        "    print(\"Error during tokenization:\", e)"
      ],
      "metadata": {
        "id": "Yapjy9vxV7p1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_encodings['input_ids'], y_train, validation_data=(test_encodings['input_ids'], y_test),\n",
        "              epochs=1, batch_size=8)  # Reduced for speed\n",
        "except Exception as e:\n",
        "    print(\"Error during training:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpS0722NWC9n",
        "outputId": "34808833-4e68-4000-971c-cb570067119c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during training: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7a02d588c6d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    test_loss, test_acc = model.evaluate(test_encodings['input_ids'], y_test)\n",
        "    print(f\"Test Accuracy: {test_acc}\")\n",
        "except Exception as e:\n",
        "    print(\"Error during evaluation:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x13kc6BzWF8W",
        "outputId": "a942f45f-6dcb-4697-d300-a3f0de713484"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during evaluation: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.\n"
          ]
        }
      ]
    }
  ]
}